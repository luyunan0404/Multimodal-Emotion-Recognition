{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two weights for the output from the each network (image and audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from scipy.io.wavfile import read as read_wav\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "from skimage import io, transform\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, audiofeature):\n",
    "        mfcc = audiofeature\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        mfcc_tensor = torch.from_numpy(mfcc)\n",
    "        mfcc_tensor = mfcc_tensor.unsqueeze(0)\n",
    "        #mfcc = mfcc.transpose((2, 0, 1))\n",
    "        return mfcc_tensor.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor_LSTM(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        mfcc, label = sample['mfcc'], sample['label']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        mfcc_tensor = torch.from_numpy(mfcc)\n",
    "        #mfcc_tensor = mfcc_tensor.unsqueeze(0)\n",
    "        #mfcc = mfcc.transpose((2, 0, 1))\n",
    "        return {'mfcc': mfcc_tensor.double(),\n",
    "                'label': torch.tensor(label).double()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audiocombination(directory):\n",
    "    features = []\n",
    "    labels = []\n",
    "    filenames_audio = []\n",
    "    for actor_number in os.listdir(directory):\n",
    "        for file in sorted(os.listdir(directory+actor_number)):\n",
    "            # load the wavefiles\n",
    "            y, _ = librosa.load(directory+actor_number+'/'+file, sr=48000, offset = 0, duration=3)  # the default sample rate for them is 16kHz, but you can also change that\n",
    "            S = librosa.feature.melspectrogram(y=y, sr=48000, n_mels=128, fmax=8000)\n",
    "            feature = librosa.feature.mfcc(S=librosa.power_to_db(S))\n",
    "\n",
    "            # truncate or zero-pad the signal\n",
    "            feature_new = np.empty((0, 282))\n",
    "            for i in range(feature.shape[0]):\n",
    "                temp = np.concatenate([feature[i], np.zeros(282-feature.shape[1])])\n",
    "                feature_new = np.append(feature_new, [temp], axis = 0)\n",
    "\n",
    "            number = file.split(\"-\")\n",
    "            emotion = number[2]\n",
    "            #if emotion == \"01\" or emotion == \"02\":\n",
    "            if emotion ==\"01\":\n",
    "                label = 4 #neutral\n",
    "            if emotion == \"02\":\n",
    "                label = 4 #\"neutral\"\n",
    "            elif emotion == \"03\":\n",
    "                label = 3 #\"happy\"\n",
    "            elif emotion == \"04\":\n",
    "                label = 5 #\"sad\"\n",
    "            elif emotion == \"05\":\n",
    "                label = 0 #\"angry\"\n",
    "            elif emotion == \"06\":\n",
    "                label = 2 #\"fearful\"\n",
    "            elif emotion == \"07\":\n",
    "                label = 1 #\"disgust\"\n",
    "            elif emotion == \"08\":\n",
    "                label = 6 #\"surprised\" \n",
    "\n",
    "            features.append(feature_new)\n",
    "            labels.append(label)\n",
    "            filenames_audio.append(file)\n",
    "\n",
    "    return features, labels, filenames_audio\n",
    "\n",
    "def imagecombination(directory, single):\n",
    "    features = []\n",
    "    filenames_image = []\n",
    "    for actor_number in os.listdir(directory):\n",
    "        for videoes in sorted(os.listdir(directory + actor_number)):\n",
    "            # if only use one frame for image network, then randomly draw one from the frames\n",
    "            if single:\n",
    "                numberoffile = len([name for name in os.listdir(directory + actor_number + '/' + videoes)])\n",
    "                #print(numberoffile)\n",
    "                index = random.randrange(1, numberoffile-1)\n",
    "                index = str(index*10)\n",
    "                target_path = directory + actor_number + '/' +videoes + '/' + index + \".jpg\"\n",
    "                image = io.imread(target_path)\n",
    "                features.append(image)\n",
    "            else:\n",
    "                multiple_image = []\n",
    "                for index in range(1,7):\n",
    "                    target_path = directory + actor_number + '/' +videoes + '/' + str(index*10) + \".jpg\"\n",
    "                    image = io.imread(target_path)\n",
    "                    multiple_image.append(image)\n",
    "                features.append(multiple_image)\n",
    "            \n",
    "            filenames_image.append(videoes)\n",
    "\n",
    "    return features, filenames_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioImage_dataset(Dataset):\n",
    "    def __init__(self, image_path, audio_path, mode, single, image_transform, audio_transform):\n",
    "\n",
    "        self.image_path = image_path\n",
    "        self.audio_path = audio_path\n",
    "        \n",
    "        self.mode = mode\n",
    "        self.single = single\n",
    "        self.image_transform = image_transform\n",
    "        self.audio_transform = audio_transform\n",
    "        \n",
    "        ## Notice the path of image and audio for the train and val is different, add mode in the path\n",
    "        self.audiofeatures, self.labels, self.filenames_audio = audiocombination(self.audio_path+self.mode+'/')\n",
    "        self.imagefeatures, self.filenames_image = imagecombination(self.image_path+self.mode+'/', self.single)\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        audiofeature = self.audiofeatures[idx]\n",
    "        transformed_audio = self.audio_transform(audiofeature)\n",
    "        imagefeature = self.imagefeatures[idx]\n",
    "        torch_list = []\n",
    "        if not self.single:\n",
    "            for single_image in imagefeature:\n",
    "                transformed_image = self.image_transform(single_image)\n",
    "                torch_list.append(transformed_image)\n",
    "            final_image = torch.stack(torch_list)\n",
    "        if self.single:\n",
    "            final_image = self.image_transform(imagefeature)\n",
    "        label = self.labels[idx]\n",
    "        filenames_audio = self.filenames_audio[idx]\n",
    "        filenames_image = self.filenames_image[idx]\n",
    "        sample = {'mfcc': transformed_audio, 'image': final_image, 'label': torch.tensor(label).double(), \n",
    "                  'filenames_audio': filenames_audio, 'filenames_image': filenames_image}\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageAudio_dataloader():\n",
    "    def __init__(self, BATCH_SIZE, single, num_workers, image_path, audio_path, image_transform, audio_transform):\n",
    "\n",
    "        self.BATCH_SIZE=BATCH_SIZE\n",
    "        self.single = single\n",
    "        self.num_workers=num_workers\n",
    "        self.image_path=image_path\n",
    "        self.audio_path=audio_path\n",
    "        self.image_transform = image_transform\n",
    "        self.audio_transform = audio_transform\n",
    "        #self.in_channel = in_channel\n",
    "        #self.frame_count ={}\n",
    "        # split the training and testing videos\n",
    "        #splitter = UCF101_splitter(path=ucf_list,split=ucf_split)\n",
    "        #self.train_video, self.test_video = splitter.split_video()\n",
    "    \n",
    "    def run(self):\n",
    "        #print(\"Now in run \")\n",
    "        #self.load_frame_count()\n",
    "        #self.get_training_dic()\n",
    "        #self.val_sample()\n",
    "        train_loader, dataset_size_train = self.train()\n",
    "        val_loader, dataset_size_valid = self.validate()\n",
    "        \n",
    "        return train_loader, val_loader, dataset_size_train, dataset_size_valid\n",
    "    \n",
    "    def train(self):\n",
    "        #print(\"Now in train\")\n",
    "        #applying trabsformation on training videos \n",
    "        \n",
    "        training_set = AudioImage_dataset(image_path=self.image_path, audio_path=self.audio_path,\n",
    "                                          mode='train', single = self.single, \n",
    "                                          image_transform = self.image_transform,\n",
    "                                          audio_transform = self.audio_transform)\n",
    "        #print('Eligible videos for training :',len(training_set),'videos')\n",
    "        dataset_size_train = len(training_set)\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            dataset=training_set, \n",
    "            batch_size=self.BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers)\n",
    "        return train_loader, dataset_size_train\n",
    "\n",
    "    def validate(self):\n",
    "        #print(\"Now in Validate\")\n",
    "        #applying transformation for validation videos \n",
    "        validation_set = AudioImage_dataset(image_path=self.image_path,audio_path=self.audio_path,\n",
    "                                            mode='valid', single = self.single, \n",
    "                                            image_transform = self.image_transform,\n",
    "                                            audio_transform = self.audio_transform)\n",
    "        dataset_size_valid = len(validation_set)\n",
    "        #print('Eligible videos for validation:',len(validation_set),'videos')\n",
    "        val_loader = DataLoader(\n",
    "            dataset=validation_set, \n",
    "            batch_size=self.BATCH_SIZE, \n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers)\n",
    "        return val_loader, dataset_size_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Setting\n",
    "batch = 16\n",
    "single = False\n",
    "image_path = \"./Multimodal-Emotion-Recognition/image_data/\"\n",
    "audio_path = \"./Multimodal-Emotion-Recognition/audio_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data_loader = ImageAudio_dataloader(BATCH_SIZE=batch, single = single, num_workers=0,\n",
    "                                image_path=image_path,       \n",
    "                                audio_path=audio_path,  \n",
    "                                image_transform = transforms.Compose([\n",
    "                                    transforms.ToPILImage(),\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                         std=[0.229, 0.224, 0.225])\n",
    "                                ]),\n",
    "                                audio_transform = transforms.Compose([\n",
    "                                    ToTensor()\n",
    "                                ]))\n",
    "train_loader, valid_loader, dataset_size_train, dataset_size_valid = data_loader.run()\n",
    "\n",
    "'''\n",
    "appending train-loader and valid loader for training the model\n",
    "'''\n",
    "fullloader = {}\n",
    "fullloader['train'] = train_loader\n",
    "fullloader['valid'] = valid_loader\n",
    "dataset_sizes = {'train': dataset_size_train, 'valid': dataset_size_valid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEST\n",
    "# for i, sample_batched in enumerate(fullloader['train']):\n",
    "#     if i == 0:\n",
    "#         image = sample_batched['image']\n",
    "#         print(image[:,1].size())\n",
    "#     if i < 5:\n",
    "#         print(i, sample_batched['image'].size(),sample_batched['mfcc'].size(),sample_batched['label'], \n",
    "#              sample_batched['filenames_image'], sample_batched['filenames_audio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "device_tensor = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer,scheduler, num_epochs, single, batch):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    epoch_loss_list = []\n",
    "    epoch_acc_list = []\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            # Iterate over data.\n",
    "            for sample_batched in fullloader[phase]:\n",
    "                image_data = sample_batched[\"image\"]\n",
    "                audio_data = sample_batched[\"mfcc\"]\n",
    "                labels = sample_batched['label']\n",
    "                # print(\"loading the data\")\n",
    "                # spat_data, temp_data, labels = data\n",
    "                \n",
    "                image_data = image_data.to(device)\n",
    "                audio_data = audio_data.to(device)\n",
    "                labels = labels.long().to(device)\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(image_data, audio_data.float(),single, batch)\n",
    "                    #print(outputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    \n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                #print(\"labels: \", labels)\n",
    "                #print(\"preds: \", preds)\n",
    "                #print(\"loss: \", loss)\n",
    "                running_loss += loss.item() * image_data.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            \n",
    "            epoch_loss_list.append(round(epoch_loss,4))\n",
    "            epoch_acc_list.append(round(epoch_acc.item(),4))\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            model.load_state_dict(best_model_wts)\n",
    "            PATH = \"./model_fusionLSTM_{}single{}.pth\".format(single, epoch)\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, epoch_loss_list, epoch_acc_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_model\n",
    "image_model = models.alexnet(pretrained=True)\n",
    "num_ftrs = image_model.classifier[6].in_features\n",
    "image_model.classifier[6] = nn.Sequential(nn.Linear(num_ftrs,7), nn.Softmax(dim = 1))\n",
    "image_model.load_state_dict(torch.load(\"./model_image_new50.pth\"))\n",
    "image_model = image_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_model CNN\n",
    "\n",
    "class Net(nn.Module):            \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 128, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(128, 128, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, 7),\n",
    "            nn.Softmax(dim = 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "audio_model = Net()\n",
    "audio_model.load_state_dict(torch.load(\"./model_audio_CNN50.pth\"))\n",
    "audio_model = audio_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio model LSTM\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_unit = 128  # number of hidden units\n",
    "        \n",
    "        # input layer\n",
    "        self.lstm_layer = nn.LSTM(input_size=20,\n",
    "                                  hidden_size=self.hidden_unit,\n",
    "                                  num_layers=2,\n",
    "                                  batch_first=True,\n",
    "                                  dropout = 0.2,\n",
    "                                  bidirectional=False)\n",
    "        \n",
    "        self.output_linear = nn.Sequential(nn.Linear(self.hidden_unit, 7),nn.Softmax(dim = 1))\n",
    "        \n",
    "    # the function for the forward pass of network (i.e. from input to output)\n",
    "    def forward(self, input):\n",
    "        # note that LSTM in Pytorch requires input shape as (batch, seq, feature)\n",
    "        # so we need to reshape the input\n",
    "        \n",
    "        input = torch.squeeze(input)\n",
    "        #print(input.size())\n",
    "        batch_size = input.size(0)\n",
    "        mfcc = input.size(1)\n",
    "        num_frame = input.size(2)\n",
    "        \n",
    "        input = input.transpose(1, 2).contiguous()  # (batch, time, freq)\n",
    "        #print(input.size())\n",
    "        \n",
    "        # pass it through layers\n",
    "        output, (h_n, c_n) = self.lstm_layer(input)  # (batch, time, hidden) # output[:,-1,:] = h_n[-1], they are all the last hidden state\n",
    "        #print(output[:,-1,:])\n",
    "        #print(h_n[-1])\n",
    "        #output = output.contiguous().view(batch_size*num_frame, -1)  # (batch*time, freq)\n",
    "        #output = self.output_linear(h_n[-1])\n",
    "        output = self.output_linear(output[:,-1,:])\n",
    "        \n",
    "        # reshape back\n",
    "        #output = output.view(batch_size, num_frame, 7)\n",
    "        #output = output.transpose(1, 2).contiguous()  # (batch_size, freq, time)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "# the model and the optimizers\n",
    "\n",
    "audio_model = LSTM()\n",
    "audio_model.load_state_dict(torch.load(\"./model_audio_LSTM50.pth\"))\n",
    "audio_model = audio_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (lstm_layer): LSTM(20, 128, num_layers=2, batch_first=True, dropout=0.2)\n",
      "  (output_linear): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=7, bias=True)\n",
      "    (1): Softmax()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(audio_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Defining a model model which will do convolution fusion of both stream and 3D pooling \n",
    "'''\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.image_feature = image_model #feature_size =   Nx512x7x7\n",
    "        self.audio_feature = audio_model #feature_size = Nx512x7x7\n",
    "        #self.layer1       = nn.Sequential(nn.Conv3d(1024, 512, 1, stride=1, padding=1, dilation=1,bias=True),\n",
    "        #                          nn.ReLU(),nn.MaxPool3d(kernel_size=2,stride=2))\n",
    "        self.fc           = nn.Sequential(nn.Linear(2,1))\n",
    "        \n",
    "    def forward(self,image_data,audio_data,single, batch):\n",
    "        transform = transforms.Compose([transforms.Normalize(mean=[0],std=[10])])\n",
    "        if not single:\n",
    "            x1 = torch.zeros(batch, 7, device=torch.device(\"cuda\"))\n",
    "            for i in range(6):\n",
    "                temp = self.image_feature(image_data[:,i])\n",
    "                temp = temp.to(device_tensor)\n",
    "                temp = torch.squeeze(transform(temp.unsqueeze(0)))\n",
    "                temp = temp.to(device)\n",
    "                x1 += temp\n",
    "            x1 = x1/6\n",
    "        if single:\n",
    "            x1 = self.image_feature(image_data)\n",
    "        \n",
    "        x2 = self.audio_feature(audio_data)\n",
    "        \n",
    "        temp = torch.cat([x1[:,0].view(16,1),x2[:,0].view(16,1)], dim = 1)\n",
    "        output = self.fc(temp)\n",
    "        for i in range(1,7):\n",
    "            temp = torch.cat([x1[:,i].view(16,1),x2[:,i].view(16,1)], dim = 1)\n",
    "            temp_output = self.fc(temp)\n",
    "            #print(temp_output)\n",
    "            output = torch.cat((output,temp_output),dim = 1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (image_feature): AlexNet(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "      (1): ReLU(inplace)\n",
      "      (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      (4): ReLU(inplace)\n",
      "      (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): ReLU(inplace)\n",
      "      (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (9): ReLU(inplace)\n",
      "      (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (11): ReLU(inplace)\n",
      "      (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "    (classifier): Sequential(\n",
      "      (0): Dropout(p=0.5)\n",
      "      (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "      (2): ReLU(inplace)\n",
      "      (3): Dropout(p=0.5)\n",
      "      (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (5): ReLU(inplace)\n",
      "      (6): Sequential(\n",
      "        (0): Linear(in_features=4096, out_features=7, bias=True)\n",
      "        (1): Softmax()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (audio_feature): LSTM(\n",
      "    (lstm_layer): LSTM(20, 128, num_layers=2, batch_first=True, dropout=0.2)\n",
      "    (output_linear): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=7, bias=True)\n",
      "      (1): Softmax()\n",
      "    )\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     print(name,param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets freeze the first few layers. This is done in two stages \n",
    "# Stage-1 Freezing all the layers \n",
    "freeze_layers = 1\n",
    "if freeze_layers:\n",
    "    for i, param in model.named_parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "# Stage-2 , Freeze all the layers till \"Conv2d_4a_3*3\"\n",
    "ct = []\n",
    "for name, param in model.named_parameters():\n",
    "    if \"audio_feature.output_linear.0.bias\" in ct:\n",
    "        param.requires_grad = True\n",
    "    ct.append(name)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t fc.0.weight\n",
      "\t fc.0.bias\n"
     ]
    }
   ],
   "source": [
    "# Create the optimizer if freeze layer before\n",
    "params_to_update = model.parameters()\n",
    "print(\"Params to learn:\")\n",
    "freeze_layers = 1\n",
    "if freeze_layers:\n",
    "    params_to_update = []\n",
    "    for name,param in model.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "            print(\"\\t\",name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "----------\n",
      "train Loss: 1.9390 Acc: 0.4633\n",
      "valid Loss: 1.9310 Acc: 0.3375\n",
      "\n",
      "Epoch 2/50\n",
      "----------\n",
      "train Loss: 1.8988 Acc: 0.5475\n",
      "valid Loss: 1.8897 Acc: 0.3375\n",
      "\n",
      "Epoch 3/50\n",
      "----------\n",
      "train Loss: 1.8216 Acc: 0.5408\n",
      "valid Loss: 1.8273 Acc: 0.3458\n",
      "\n",
      "Epoch 4/50\n",
      "----------\n",
      "train Loss: 1.7198 Acc: 0.5325\n",
      "valid Loss: 1.7650 Acc: 0.3458\n",
      "\n",
      "Epoch 5/50\n",
      "----------\n",
      "train Loss: 1.5970 Acc: 0.5375\n",
      "valid Loss: 1.7388 Acc: 0.3458\n",
      "\n",
      "Epoch 6/50\n",
      "----------\n",
      "train Loss: 1.4842 Acc: 0.5292\n",
      "valid Loss: 1.7719 Acc: 0.3458\n",
      "\n",
      "Epoch 7/50\n",
      "----------\n",
      "train Loss: 1.4274 Acc: 0.5392\n",
      "valid Loss: 1.8910 Acc: 0.3375\n",
      "\n",
      "Epoch 8/50\n",
      "----------\n",
      "train Loss: 1.3964 Acc: 0.5442\n",
      "valid Loss: 2.0644 Acc: 0.3500\n",
      "\n",
      "Epoch 9/50\n",
      "----------\n",
      "train Loss: 1.4139 Acc: 0.5508\n",
      "valid Loss: 2.3049 Acc: 0.3500\n",
      "\n",
      "Epoch 10/50\n",
      "----------\n",
      "train Loss: 1.4532 Acc: 0.5458\n",
      "valid Loss: 2.5769 Acc: 0.3375\n",
      "\n",
      "Epoch 11/50\n",
      "----------\n",
      "train Loss: 1.4028 Acc: 0.5408\n",
      "valid Loss: 2.0877 Acc: 0.3417\n",
      "\n",
      "Epoch 12/50\n",
      "----------\n",
      "train Loss: 1.3923 Acc: 0.5383\n",
      "valid Loss: 2.1251 Acc: 0.3417\n",
      "\n",
      "Epoch 13/50\n",
      "----------\n",
      "train Loss: 1.3892 Acc: 0.5425\n",
      "valid Loss: 2.1401 Acc: 0.3333\n",
      "\n",
      "Epoch 14/50\n",
      "----------\n",
      "train Loss: 1.3724 Acc: 0.5558\n",
      "valid Loss: 2.1282 Acc: 0.3500\n",
      "\n",
      "Epoch 15/50\n",
      "----------\n",
      "train Loss: 1.4001 Acc: 0.5425\n",
      "valid Loss: 2.1651 Acc: 0.3375\n",
      "\n",
      "Epoch 16/50\n",
      "----------\n",
      "train Loss: 1.4081 Acc: 0.5442\n",
      "valid Loss: 2.2009 Acc: 0.3375\n",
      "\n",
      "Epoch 17/50\n",
      "----------\n",
      "train Loss: 1.3906 Acc: 0.5475\n",
      "valid Loss: 2.1804 Acc: 0.3375\n",
      "\n",
      "Epoch 18/50\n",
      "----------\n",
      "train Loss: 1.3896 Acc: 0.5542\n",
      "valid Loss: 2.2102 Acc: 0.3542\n",
      "\n",
      "Epoch 19/50\n",
      "----------\n",
      "train Loss: 1.4184 Acc: 0.5325\n",
      "valid Loss: 2.2518 Acc: 0.3375\n",
      "\n",
      "Epoch 20/50\n",
      "----------\n",
      "train Loss: 1.4183 Acc: 0.5367\n",
      "valid Loss: 2.2607 Acc: 0.3458\n",
      "\n",
      "Epoch 21/50\n",
      "----------\n",
      "train Loss: 1.3994 Acc: 0.5567\n",
      "valid Loss: 2.2240 Acc: 0.3417\n",
      "\n",
      "Epoch 22/50\n",
      "----------\n",
      "train Loss: 1.3930 Acc: 0.5517\n",
      "valid Loss: 2.2185 Acc: 0.3500\n",
      "\n",
      "Epoch 23/50\n",
      "----------\n",
      "train Loss: 1.4130 Acc: 0.5425\n",
      "valid Loss: 2.2406 Acc: 0.3458\n",
      "\n",
      "Epoch 24/50\n",
      "----------\n",
      "train Loss: 1.3969 Acc: 0.5542\n",
      "valid Loss: 2.2489 Acc: 0.3500\n",
      "\n",
      "Epoch 25/50\n",
      "----------\n",
      "train Loss: 1.3788 Acc: 0.5558\n",
      "valid Loss: 2.2194 Acc: 0.3333\n",
      "\n",
      "Epoch 26/50\n",
      "----------\n",
      "train Loss: 1.4152 Acc: 0.5383\n",
      "valid Loss: 2.2296 Acc: 0.3417\n",
      "\n",
      "Epoch 27/50\n",
      "----------\n",
      "train Loss: 1.3985 Acc: 0.5467\n",
      "valid Loss: 2.2627 Acc: 0.3417\n",
      "\n",
      "Epoch 28/50\n",
      "----------\n",
      "train Loss: 1.3980 Acc: 0.5475\n",
      "valid Loss: 2.2362 Acc: 0.3500\n",
      "\n",
      "Epoch 29/50\n",
      "----------\n",
      "train Loss: 1.3733 Acc: 0.5583\n",
      "valid Loss: 2.2336 Acc: 0.3417\n",
      "\n",
      "Epoch 30/50\n",
      "----------\n",
      "train Loss: 1.3957 Acc: 0.5417\n",
      "valid Loss: 2.2473 Acc: 0.3375\n",
      "\n",
      "Epoch 31/50\n",
      "----------\n",
      "train Loss: 1.4041 Acc: 0.5433\n",
      "valid Loss: 2.2193 Acc: 0.3542\n",
      "\n",
      "Epoch 32/50\n",
      "----------\n",
      "train Loss: 1.4076 Acc: 0.5550\n",
      "valid Loss: 2.2212 Acc: 0.3292\n",
      "\n",
      "Epoch 33/50\n",
      "----------\n",
      "train Loss: 1.4220 Acc: 0.5375\n",
      "valid Loss: 2.2204 Acc: 0.3417\n",
      "\n",
      "Epoch 34/50\n",
      "----------\n",
      "train Loss: 1.4385 Acc: 0.5358\n",
      "valid Loss: 2.2167 Acc: 0.3500\n",
      "\n",
      "Epoch 35/50\n",
      "----------\n",
      "train Loss: 1.4035 Acc: 0.5442\n",
      "valid Loss: 2.2106 Acc: 0.3458\n",
      "\n",
      "Epoch 36/50\n",
      "----------\n",
      "train Loss: 1.3855 Acc: 0.5475\n",
      "valid Loss: 2.2446 Acc: 0.3375\n",
      "\n",
      "Epoch 37/50\n",
      "----------\n",
      "train Loss: 1.4010 Acc: 0.5517\n",
      "valid Loss: 2.2320 Acc: 0.3375\n",
      "\n",
      "Epoch 38/50\n",
      "----------\n",
      "train Loss: 1.4093 Acc: 0.5525\n",
      "valid Loss: 2.2356 Acc: 0.3333\n",
      "\n",
      "Epoch 39/50\n",
      "----------\n",
      "train Loss: 1.3878 Acc: 0.5542\n",
      "valid Loss: 2.2249 Acc: 0.3458\n",
      "\n",
      "Epoch 40/50\n",
      "----------\n",
      "train Loss: 1.4146 Acc: 0.5533\n",
      "valid Loss: 2.2128 Acc: 0.3458\n",
      "\n",
      "Epoch 41/50\n",
      "----------\n",
      "train Loss: 1.4204 Acc: 0.5358\n",
      "valid Loss: 2.2376 Acc: 0.3417\n",
      "\n",
      "Epoch 42/50\n",
      "----------\n",
      "train Loss: 1.3933 Acc: 0.5608\n",
      "valid Loss: 2.2191 Acc: 0.3417\n",
      "\n",
      "Epoch 43/50\n",
      "----------\n",
      "train Loss: 1.3993 Acc: 0.5408\n",
      "valid Loss: 2.2284 Acc: 0.3500\n",
      "\n",
      "Epoch 44/50\n",
      "----------\n",
      "train Loss: 1.3727 Acc: 0.5492\n",
      "valid Loss: 2.2126 Acc: 0.3583\n",
      "\n",
      "Epoch 45/50\n",
      "----------\n",
      "train Loss: 1.3954 Acc: 0.5375\n",
      "valid Loss: 2.2365 Acc: 0.3500\n",
      "\n",
      "Epoch 46/50\n",
      "----------\n",
      "train Loss: 1.4003 Acc: 0.5600\n",
      "valid Loss: 2.2263 Acc: 0.3500\n",
      "\n",
      "Epoch 47/50\n",
      "----------\n",
      "train Loss: 1.3953 Acc: 0.5500\n",
      "valid Loss: 2.2091 Acc: 0.3417\n",
      "\n",
      "Epoch 48/50\n",
      "----------\n",
      "train Loss: 1.4057 Acc: 0.5442\n",
      "valid Loss: 2.2129 Acc: 0.3458\n",
      "\n",
      "Epoch 49/50\n",
      "----------\n",
      "train Loss: 1.4022 Acc: 0.5342\n",
      "valid Loss: 2.2411 Acc: 0.3500\n",
      "\n",
      "Epoch 50/50\n",
      "----------\n",
      "train Loss: 1.3967 Acc: 0.5467\n",
      "valid Loss: 2.1923 Acc: 0.3542\n",
      "\n",
      "Training complete in 24m 7s\n",
      "Best val Acc: 0.358333\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 10 epochs\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "\n",
    "model, loss_list, acc_list = train_model(model, criterion, optimizer,scheduler, num_epochs=50, single = single, batch = batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"./fusion3_LSTM_acc.txt\"\n",
    "import json\n",
    "with open(filepath, 'w') as f:\n",
    "    f.write(json.dumps(acc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"./fusion3_LSTM_loss.txt\"\n",
    "import json\n",
    "with open(filepath, 'w') as f:\n",
    "    f.write(json.dumps(loss_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[24.4127,  3.2912]], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.fc[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros(5, 7, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
