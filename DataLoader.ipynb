{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, audiofeature):\n",
    "        mfcc = audiofeature\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        mfcc_tensor = torch.from_numpy(mfcc)\n",
    "        mfcc_tensor = mfcc_tensor.unsqueeze(0)\n",
    "        #mfcc = mfcc.transpose((2, 0, 1))\n",
    "        return mfcc_tensor.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audiocombination(directory):\n",
    "    features = []\n",
    "    labels = []\n",
    "    filenames_audio = []\n",
    "    for actor_number in os.listdir(directory):\n",
    "        for file in sorted(os.listdir(directory+actor_number)):\n",
    "            # load the wavefiles\n",
    "            y, _ = librosa.load(directory+actor_number+'/'+file, sr=48000, offset = 0, duration=3)  # the default sample rate for them is 16kHz, but you can also change that\n",
    "            S = librosa.feature.melspectrogram(y=y, sr=48000, n_mels=128, fmax=8000)\n",
    "            feature = librosa.feature.mfcc(S=librosa.power_to_db(S))\n",
    "\n",
    "            # truncate or zero-pad the signal\n",
    "            feature_new = np.empty((0, 282))\n",
    "            for i in range(feature.shape[0]):\n",
    "                temp = np.concatenate([feature[i], np.zeros(282-feature.shape[1])])\n",
    "                feature_new = np.append(feature_new, [temp], axis = 0)\n",
    "\n",
    "            number = file.split(\"-\")\n",
    "            emotion = number[2]\n",
    "            #if emotion == \"01\" or emotion == \"02\":\n",
    "            if emotion ==\"01\":\n",
    "                label = 4 #neutral\n",
    "            if emotion == \"02\":\n",
    "                label = 4 #\"neutral\"\n",
    "            elif emotion == \"03\":\n",
    "                label = 3 #\"happy\"\n",
    "            elif emotion == \"04\":\n",
    "                label = 5 #\"sad\"\n",
    "            elif emotion == \"05\":\n",
    "                label = 0 #\"angry\"\n",
    "            elif emotion == \"06\":\n",
    "                label = 2 #\"fearful\"\n",
    "            elif emotion == \"07\":\n",
    "                label = 1 #\"disgust\"\n",
    "            elif emotion == \"08\":\n",
    "                label = 6 #\"surprised\" \n",
    "\n",
    "            features.append(feature_new)\n",
    "            labels.append(label)\n",
    "            filenames_audio.append(file)\n",
    "\n",
    "    return features, labels, filenames_audio\n",
    "\n",
    "def imagecombination(directory, single):\n",
    "    features = []\n",
    "    filenames_image = []\n",
    "    for actor_number in os.listdir(directory):\n",
    "        for videoes in sorted(os.listdir(directory + actor_number)):\n",
    "            # if only use one frame for image network, then randomly draw one from the frames\n",
    "            if single:\n",
    "                numberoffile = len([name for name in os.listdir(directory + actor_number + '/' + videoes)])\n",
    "                #print(numberoffile)\n",
    "                index = random.randrange(1, numberoffile-1)\n",
    "                index = str(index*10)\n",
    "                target_path = directory + actor_number + '/' +videoes + '/' + index + \".jpg\"\n",
    "                image = io.imread(target_path)\n",
    "                features.append(image)\n",
    "            \n",
    "                filenames_image.append(videoes)\n",
    "\n",
    "    return features, filenames_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioImage_dataset(Dataset):\n",
    "    def __init__(self, image_path, audio_path, mode, single, image_transform, audio_transform):\n",
    "\n",
    "        self.image_path = image_path\n",
    "        self.audio_path = audio_path\n",
    "        \n",
    "        self.mode = mode\n",
    "        self.single = single\n",
    "        self.image_transform = image_transform\n",
    "        self.audio_transform = audio_transform\n",
    "        \n",
    "        ## Notice the path of image and audio for the train and val is different, add mode in the path\n",
    "        self.audiofeatures, self.labels, self.filenames_audio = audiocombination(self.audio_path+self.mode+'/')\n",
    "        self.imagefeatures, self.filenames_image = imagecombination(self.image_path+self.mode+'/', self.single)\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        audiofeature = self.audiofeatures[idx]\n",
    "        transformed_audio = self.audio_transform(audiofeature)\n",
    "        imagefeature = self.imagefeatures[idx]\n",
    "        transformed_image = self.image_transform(imagefeature)\n",
    "        label = self.labels[idx]\n",
    "        filenames_audio = self.filenames_audio[idx]\n",
    "        filenames_image = self.filenames_image[idx]\n",
    "        sample = {'mfcc': transformed_audio, 'image': transformed_image, 'label': torch.tensor(label).double(), \n",
    "                  'filenames_audio': filenames_audio, 'filenames_image': filenames_image}\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageAudio_dataloader():\n",
    "    def __init__(self, BATCH_SIZE, single, num_workers, image_path, audio_path, image_transform, audio_transform):\n",
    "\n",
    "        self.BATCH_SIZE=BATCH_SIZE\n",
    "        self.single = single\n",
    "        self.num_workers=num_workers\n",
    "        self.image_path=image_path\n",
    "        self.audio_path=audio_path\n",
    "        self.image_transform = image_transform\n",
    "        self.audio_transform = audio_transform\n",
    "        #self.in_channel = in_channel\n",
    "        #self.frame_count ={}\n",
    "        # split the training and testing videos\n",
    "        #splitter = UCF101_splitter(path=ucf_list,split=ucf_split)\n",
    "        #self.train_video, self.test_video = splitter.split_video()\n",
    "    \n",
    "    def run(self):\n",
    "        #print(\"Now in run \")\n",
    "        #self.load_frame_count()\n",
    "        #self.get_training_dic()\n",
    "        #self.val_sample()\n",
    "        train_loader, dataset_size_train = self.train()\n",
    "        val_loader, dataset_size_valid = self.validate()\n",
    "        \n",
    "        return train_loader, val_loader, dataset_size_train, dataset_size_valid\n",
    "    \n",
    "    def train(self):\n",
    "        #print(\"Now in train\")\n",
    "        #applying trabsformation on training videos \n",
    "        \n",
    "        training_set = AudioImage_dataset(image_path=self.image_path, audio_path=self.audio_path,\n",
    "                                          mode='train', single = self.single, \n",
    "                                          image_transform = self.image_transform,\n",
    "                                          audio_transform = self.audio_transform)\n",
    "        #print('Eligible videos for training :',len(training_set),'videos')\n",
    "        dataset_size_train = len(training_set)\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            dataset=training_set, \n",
    "            batch_size=self.BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers)\n",
    "        return train_loader, dataset_size_train\n",
    "\n",
    "    def validate(self):\n",
    "        #print(\"Now in Validate\")\n",
    "        #applying transformation for validation videos \n",
    "        validation_set = AudioImage_dataset(image_path=self.image_path,audio_path=self.audio_path,\n",
    "                                            mode='valid', single = self.single, \n",
    "                                            image_transform = self.image_transform,\n",
    "                                            audio_transform = self.audio_transform)\n",
    "        dataset_size_valid = len(validation_set)\n",
    "        #print('Eligible videos for validation:',len(validation_set),'videos')\n",
    "        val_loader = DataLoader(\n",
    "            dataset=validation_set, \n",
    "            batch_size=self.BATCH_SIZE, \n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers)\n",
    "        return val_loader, dataset_size_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the train and test data\n",
    "'''\n",
    "change here for loading data for spatial loader, temporal loader\n",
    "'''\n",
    "data_loader = ImageAudio_dataloader(BATCH_SIZE=16, single = True, num_workers=0,\n",
    "                                image_path='./Multimodal-Emotion-Recognition/image_data/',  # path for image data       \n",
    "                                audio_path='./Multimodal-Emotion-Recognition/audio_data/',  # path for audio data\n",
    "                                image_transform = transforms.Compose([\n",
    "                                    transforms.ToPILImage(),\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                         std=[0.229, 0.224, 0.225])\n",
    "                                ]),\n",
    "                                audio_transform = transforms.Compose([\n",
    "                                    ToTensor()\n",
    "                                ]))\n",
    "train_loader, valid_loader, dataset_size_train, dataset_size_valid = data_loader.run()\n",
    "\n",
    "'''\n",
    "appending train-loader and valid loader for training the model\n",
    "'''\n",
    "fullloader = {}\n",
    "fullloader['train'] = train_loader\n",
    "fullloader['valid'] = valid_loader\n",
    "dataset_sizes = {'train': dataset_size_train, 'valid': dataset_size_valid}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
