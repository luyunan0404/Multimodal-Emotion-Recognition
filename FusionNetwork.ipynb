{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from scipy.io.wavfile import read as read_wav\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "#import cv2\n",
    "#import face_recognition\n",
    "from PIL import Image\n",
    "from skimage import io, transform\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, audiofeature):\n",
    "        mfcc = audiofeature\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        mfcc_tensor = torch.from_numpy(mfcc)\n",
    "        mfcc_tensor = mfcc_tensor.unsqueeze(0)\n",
    "        #mfcc = mfcc.transpose((2, 0, 1))\n",
    "        return mfcc_tensor.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audiocombination(directory):\n",
    "    features = []\n",
    "    labels = []\n",
    "    filenames_audio = []\n",
    "    for actor_number in os.listdir(directory):\n",
    "        for file in sorted(os.listdir(directory+actor_number)):\n",
    "            # load the wavefiles\n",
    "            y, _ = librosa.load(directory+actor_number+'/'+file, sr=48000, offset = 0, duration=3)  # the default sample rate for them is 16kHz, but you can also change that\n",
    "            S = librosa.feature.melspectrogram(y=y, sr=48000, n_mels=128, fmax=8000)\n",
    "            feature = librosa.feature.mfcc(S=librosa.power_to_db(S))\n",
    "\n",
    "            # truncate or zero-pad the signal\n",
    "            feature_new = np.empty((0, 282))\n",
    "            for i in range(feature.shape[0]):\n",
    "                temp = np.concatenate([feature[i], np.zeros(282-feature.shape[1])])\n",
    "                feature_new = np.append(feature_new, [temp], axis = 0)\n",
    "\n",
    "            number = file.split(\"-\")\n",
    "            emotion = number[2]\n",
    "            #if emotion == \"01\" or emotion == \"02\":\n",
    "            if emotion ==\"01\":\n",
    "                label = 4 #neutral\n",
    "            if emotion == \"02\":\n",
    "                label = 4 #\"neutral\"\n",
    "            elif emotion == \"03\":\n",
    "                label = 3 #\"happy\"\n",
    "            elif emotion == \"04\":\n",
    "                label = 5 #\"sad\"\n",
    "            elif emotion == \"05\":\n",
    "                label = 0 #\"angry\"\n",
    "            elif emotion == \"06\":\n",
    "                label = 2 #\"fearful\"\n",
    "            elif emotion == \"07\":\n",
    "                label = 1 #\"disgust\"\n",
    "            elif emotion == \"08\":\n",
    "                label = 6 #\"surprised\" \n",
    "\n",
    "            features.append(feature_new)\n",
    "            labels.append(label)\n",
    "            filenames_audio.append(file)\n",
    "\n",
    "    return features, labels, filenames_audio\n",
    "\n",
    "def imagecombination(directory, single):\n",
    "    features = []\n",
    "    filenames_image = []\n",
    "    for actor_number in os.listdir(directory):\n",
    "        for videoes in sorted(os.listdir(directory + actor_number)):\n",
    "            # if only use one frame for image network, then randomly draw one from the frames\n",
    "            if single:\n",
    "                numberoffile = len([name for name in os.listdir(directory + actor_number + '/' + videoes)])\n",
    "                #print(numberoffile)\n",
    "                index = random.randrange(1, numberoffile-1)\n",
    "                index = str(index*10)\n",
    "                target_path = directory + actor_number + '/' +videoes + '/' + index + \".jpg\"\n",
    "                image = io.imread(target_path)\n",
    "                features.append(image)\n",
    "            \n",
    "                filenames_image.append(videoes)\n",
    "\n",
    "    return features, filenames_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioImage_dataset(Dataset):\n",
    "    def __init__(self, image_path, audio_path, mode, single, image_transform, audio_transform):\n",
    "\n",
    "        self.image_path = image_path\n",
    "        self.audio_path = audio_path\n",
    "        \n",
    "        self.mode = mode\n",
    "        self.single = single\n",
    "        self.image_transform = image_transform\n",
    "        self.audio_transform = audio_transform\n",
    "        \n",
    "        ## Notice the path of image and audio for the train and val is different, add mode in the path\n",
    "        self.audiofeatures, self.labels, self.filenames_audio = audiocombination(self.audio_path+self.mode+'/')\n",
    "        self.imagefeatures, self.filenames_image = imagecombination(self.image_path+self.mode+'/', self.single)\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        audiofeature = self.audiofeatures[idx]\n",
    "        transformed_audio = self.audio_transform(audiofeature)\n",
    "        imagefeature = self.imagefeatures[idx]\n",
    "        transformed_image = self.image_transform(imagefeature)\n",
    "        label = self.labels[idx]\n",
    "        filenames_audio = self.filenames_audio[idx]\n",
    "        filenames_image = self.filenames_image[idx]\n",
    "        sample = {'mfcc': transformed_audio, 'image': transformed_image, 'label': torch.tensor(label).double(), \n",
    "                  'filenames_audio': filenames_audio, 'filenames_image': filenames_image}\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageAudio_dataloader():\n",
    "    def __init__(self, BATCH_SIZE, single, num_workers, image_path, audio_path, image_transform, audio_transform):\n",
    "\n",
    "        self.BATCH_SIZE=BATCH_SIZE\n",
    "        self.single = single\n",
    "        self.num_workers=num_workers\n",
    "        self.image_path=image_path\n",
    "        self.audio_path=audio_path\n",
    "        self.image_transform = image_transform\n",
    "        self.audio_transform = audio_transform\n",
    "        #self.in_channel = in_channel\n",
    "        #self.frame_count ={}\n",
    "        # split the training and testing videos\n",
    "        #splitter = UCF101_splitter(path=ucf_list,split=ucf_split)\n",
    "        #self.train_video, self.test_video = splitter.split_video()\n",
    "    \n",
    "    def run(self):\n",
    "        #print(\"Now in run \")\n",
    "        #self.load_frame_count()\n",
    "        #self.get_training_dic()\n",
    "        #self.val_sample()\n",
    "        train_loader, dataset_size_train = self.train()\n",
    "        val_loader, dataset_size_valid = self.validate()\n",
    "        \n",
    "        return train_loader, val_loader, dataset_size_train, dataset_size_valid\n",
    "    \n",
    "    def train(self):\n",
    "        #print(\"Now in train\")\n",
    "        #applying trabsformation on training videos \n",
    "        \n",
    "        training_set = AudioImage_dataset(image_path=self.image_path, audio_path=self.audio_path,\n",
    "                                          mode='train', single = self.single, \n",
    "                                          image_transform = self.image_transform,\n",
    "                                          audio_transform = self.audio_transform)\n",
    "        #print('Eligible videos for training :',len(training_set),'videos')\n",
    "        dataset_size_train = len(training_set)\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            dataset=training_set, \n",
    "            batch_size=self.BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers)\n",
    "        return train_loader, dataset_size_train\n",
    "\n",
    "    def validate(self):\n",
    "        #print(\"Now in Validate\")\n",
    "        #applying transformation for validation videos \n",
    "        validation_set = AudioImage_dataset(image_path=self.image_path,audio_path=self.audio_path,\n",
    "                                            mode='valid', single = self.single, \n",
    "                                            image_transform = self.image_transform,\n",
    "                                            audio_transform = self.audio_transform)\n",
    "        dataset_size_valid = len(validation_set)\n",
    "        #print('Eligible videos for validation:',len(validation_set),'videos')\n",
    "        val_loader = DataLoader(\n",
    "            dataset=validation_set, \n",
    "            batch_size=self.BATCH_SIZE, \n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers)\n",
    "        return val_loader, dataset_size_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the train and test data\n",
    "'''\n",
    "change here for loading data for spatial loader, temporal loader\n",
    "'''\n",
    "data_loader = ImageAudio_dataloader(BATCH_SIZE=16, single = True, num_workers=0,\n",
    "                                image_path='./Multimodal-Emotion-Recognition/image_data/',  # path for image data       \n",
    "                                audio_path='./Multimodal-Emotion-Recognition/audio_data/',  # path for audio data\n",
    "                                image_transform = transforms.Compose([\n",
    "                                    transforms.ToPILImage(),\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                         std=[0.229, 0.224, 0.225])\n",
    "                                ]),\n",
    "                                audio_transform = transforms.Compose([\n",
    "                                    ToTensor()\n",
    "                                ]))\n",
    "train_loader, valid_loader, dataset_size_train, dataset_size_valid = data_loader.run()\n",
    "\n",
    "'''\n",
    "appending train-loader and valid loader for training the model\n",
    "'''\n",
    "fullloader = {}\n",
    "fullloader['train'] = train_loader\n",
    "fullloader['valid'] = valid_loader\n",
    "dataset_sizes = {'train': dataset_size_train, 'valid': dataset_size_valid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, sample_batched in enumerate(fullloader['train']):\n",
    "#     if i < 5:\n",
    "#         print(i, sample_batched['image'].size(),sample_batched['mfcc'].size(),sample_batched['label'], \n",
    "#              sample_batched['filenames_image'], sample_batched['filenames_audio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer,scheduler, num_epochs):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    epoch_loss_list = []\n",
    "    epoch_acc_list = []\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            # Iterate over data.\n",
    "            for sample_batched in fullloader[phase]:\n",
    "                image_data = sample_batched[\"image\"]\n",
    "                audio_data = sample_batched[\"mfcc\"]\n",
    "                labels = sample_batched['label']\n",
    "                \n",
    "                image_data = image_data.to(device)\n",
    "                audio_data = audio_data.to(device)\n",
    "                labels = labels.long().to(device)\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(image_data, audio_data.float())\n",
    "                    #print(outputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    \n",
    "                    loss = criterion(outputs, labels)\n",
    "                    #print(\"loss: \", loss)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                #print(\"labels: \", labels)\n",
    "                #print(\"preds: \", preds)\n",
    "                running_loss += loss.item() * image_data.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            \n",
    "            epoch_loss_list.append(round(epoch_loss,4))\n",
    "            epoch_acc_list.append(round(epoch_acc.item(),4))\n",
    "            \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            model.load_state_dict(best_model_wts)\n",
    "            PATH = \"./model_fusion1_CNN{}.pth\".format(epoch)\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, epoch_loss_list, epoch_acc_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_model\n",
    "image_temp = models.alexnet(pretrained=True)\n",
    "\n",
    "num_ftrs = image_temp.classifier[6].in_features\n",
    "image_temp.classifier[6] = nn.Sequential(nn.Linear(num_ftrs,7), nn.Softmax(dim = 1))\n",
    "image_temp.load_state_dict(torch.load(\"./model_image_new50.pth\"))\n",
    "image_temp.classifier = nn.Sequential(*list(image_temp.classifier.children())[:-2])\n",
    "image_model = image_temp.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_model\n",
    "\n",
    "class Net(nn.Module):            \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 128, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(128, 128, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, 7),\n",
    "            nn.Softmax(dim = 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "audio_model = Net()\n",
    "audio_model.load_state_dict(torch.load(\"./model_audio_CNN50.pth\"))\n",
    "audio_model.classifier = nn.Sequential(*list(audio_model.classifier.children())[:-2])\n",
    "audio_model = audio_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_tensor = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Defining a model model which will do convolution fusion of both stream and 3D pooling \n",
    "'''\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.image_feature = image_model #feature_size =   Nx512x7x7\n",
    "        self.audio_feature = audio_model #feature_size = Nx512x7x7\n",
    "        #self.layer1       = nn.Sequential(nn.Conv3d(1024, 512, 1, stride=1, padding=1, dilation=1,bias=True),\n",
    "        #                          nn.ReLU(),nn.MaxPool3d(kernel_size=2,stride=2))\n",
    "        self.fc           = nn.Sequential(nn.Linear(8192,4096), nn.ReLU(), nn.Dropout(p=0.85),\n",
    "                                        nn.Linear(4096, 1024), nn.ReLU(), nn.Dropout(p=0.85),\n",
    "                                        nn.Linear(1024, 7))\n",
    "        \n",
    "    def forward(self,image_data,audio_data):\n",
    "        x1       = self.image_feature(image_data)\n",
    "        x2       = self.audio_feature(audio_data)\n",
    "        x1 = x1.to(device_tensor)\n",
    "        x2 = x2.to(device_tensor)\n",
    "        transform = transforms.Compose([transforms.Normalize(mean=[0],std=[40])])\n",
    "        x1_new = torch.squeeze(transform(x1.unsqueeze(0)))\n",
    "        x2_new = torch.squeeze(transform(x2.unsqueeze(0)))\n",
    "        \n",
    "        \n",
    "        y        = torch.cat((x1_new,x2_new), dim= 1)\n",
    "        y = y.to(device)\n",
    "        out      = self.fc(y)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (image_feature): AlexNet(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "      (1): ReLU(inplace)\n",
      "      (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      (4): ReLU(inplace)\n",
      "      (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): ReLU(inplace)\n",
      "      (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (9): ReLU(inplace)\n",
      "      (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (11): ReLU(inplace)\n",
      "      (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "    (classifier): Sequential(\n",
      "      (0): Dropout(p=0.5)\n",
      "      (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "      (2): ReLU(inplace)\n",
      "      (3): Dropout(p=0.5)\n",
      "      (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (audio_feature): Net(\n",
      "    (features): Sequential(\n",
      "      (0): Conv2d(1, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      (1): ReLU(inplace)\n",
      "      (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (3): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "      (4): ReLU(inplace)\n",
      "      (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): ReLU(inplace)\n",
      "      (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (9): ReLU(inplace)\n",
      "      (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (11): ReLU(inplace)\n",
      "      (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "    (classifier): Sequential(\n",
      "      (0): Dropout(p=0.5)\n",
      "      (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "      (2): ReLU(inplace)\n",
      "      (3): Dropout(p=0.5)\n",
      "      (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "      (5): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=8192, out_features=4096, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.85)\n",
      "    (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.85)\n",
      "    (6): Linear(in_features=1024, out_features=7, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets freeze the first few layers. This is done in two stages \n",
    "# Stage-1 Freezing all the layers \n",
    "freeze_layers = 1\n",
    "if freeze_layers:\n",
    "    for i, param in model.named_parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "# Stage-2 , Freeze all the layers till \"Conv2d_4a_3*3\"\n",
    "ct = []\n",
    "for name, param in model.named_parameters():\n",
    "    if \"audio_feature.classifier.4.bias\" in ct:\n",
    "        param.requires_grad = True\n",
    "    ct.append(name)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t fc.0.weight\n",
      "\t fc.0.bias\n",
      "\t fc.3.weight\n",
      "\t fc.3.bias\n",
      "\t fc.6.weight\n",
      "\t fc.6.bias\n"
     ]
    }
   ],
   "source": [
    "# Create the optimizer if freeze layer before\n",
    "params_to_update = model.parameters()\n",
    "print(\"Params to learn:\")\n",
    "freeze_layers = 1\n",
    "if freeze_layers:\n",
    "    params_to_update = []\n",
    "    for name,param in model.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "            print(\"\\t\",name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "----------\n",
      "train Loss: 136.7298 Acc: 0.1542\n",
      "valid Loss: 887.2625 Acc: 0.2000\n",
      "\n",
      "Epoch 2/50\n",
      "----------\n",
      "train Loss: 99756.7852 Acc: 0.1550\n",
      "valid Loss: 1420.9247 Acc: 0.2000\n",
      "\n",
      "Epoch 3/50\n",
      "----------\n",
      "train Loss: 1226544.4846 Acc: 0.1625\n",
      "valid Loss: 81.4671 Acc: 0.1333\n",
      "\n",
      "Epoch 4/50\n",
      "----------\n",
      "train Loss: 782.9441 Acc: 0.1267\n",
      "valid Loss: 15.1208 Acc: 0.2000\n",
      "\n",
      "Epoch 5/50\n",
      "----------\n",
      "train Loss: 24.2548 Acc: 0.1667\n",
      "valid Loss: 4.4202 Acc: 0.2000\n",
      "\n",
      "Epoch 6/50\n",
      "----------\n",
      "train Loss: 6.2928 Acc: 0.1383\n",
      "valid Loss: 7.8524 Acc: 0.1333\n",
      "\n",
      "Epoch 7/50\n",
      "----------\n",
      "train Loss: 7.8346 Acc: 0.1400\n",
      "valid Loss: 9.1097 Acc: 0.1333\n",
      "\n",
      "Epoch 8/50\n",
      "----------\n",
      "train Loss: 67.3004 Acc: 0.1217\n",
      "valid Loss: 4.6148 Acc: 0.1333\n",
      "\n",
      "Epoch 9/50\n",
      "----------\n",
      "train Loss: 8.8773 Acc: 0.1650\n",
      "valid Loss: 13.9818 Acc: 0.1333\n",
      "\n",
      "Epoch 10/50\n",
      "----------\n",
      "train Loss: 283.7111 Acc: 0.1758\n",
      "valid Loss: 11.1580 Acc: 0.2000\n",
      "\n",
      "Epoch 11/50\n",
      "----------\n",
      "train Loss: 631.1417 Acc: 0.1400\n",
      "valid Loss: 2.2362 Acc: 0.1333\n",
      "\n",
      "Epoch 12/50\n",
      "----------\n",
      "train Loss: 2.7858 Acc: 0.1458\n",
      "valid Loss: 2.7189 Acc: 0.1333\n",
      "\n",
      "Epoch 13/50\n",
      "----------\n",
      "train Loss: 2.9328 Acc: 0.1325\n",
      "valid Loss: 2.9052 Acc: 0.1333\n",
      "\n",
      "Epoch 14/50\n",
      "----------\n",
      "train Loss: 2.7227 Acc: 0.1233\n",
      "valid Loss: 2.5270 Acc: 0.1333\n",
      "\n",
      "Epoch 15/50\n",
      "----------\n",
      "train Loss: 2.3764 Acc: 0.1325\n",
      "valid Loss: 2.2096 Acc: 0.1333\n",
      "\n",
      "Epoch 16/50\n",
      "----------\n",
      "train Loss: 2.2738 Acc: 0.1425\n",
      "valid Loss: 2.5792 Acc: 0.2000\n",
      "\n",
      "Epoch 17/50\n",
      "----------\n",
      "train Loss: 3.0541 Acc: 0.2000\n",
      "valid Loss: 3.3264 Acc: 0.2000\n",
      "\n",
      "Epoch 18/50\n",
      "----------\n",
      "train Loss: 3.0821 Acc: 0.1892\n",
      "valid Loss: 2.7223 Acc: 0.1333\n",
      "\n",
      "Epoch 19/50\n",
      "----------\n",
      "train Loss: 2.3384 Acc: 0.1350\n",
      "valid Loss: 2.1145 Acc: 0.1333\n",
      "\n",
      "Epoch 20/50\n",
      "----------\n",
      "train Loss: 2.4623 Acc: 0.1333\n",
      "valid Loss: 2.8777 Acc: 0.1333\n",
      "\n",
      "Epoch 21/50\n",
      "----------\n",
      "train Loss: 371.0876 Acc: 0.1608\n",
      "valid Loss: 1.9456 Acc: 0.1333\n",
      "\n",
      "Epoch 22/50\n",
      "----------\n",
      "train Loss: 1.9534 Acc: 0.1383\n",
      "valid Loss: 1.9599 Acc: 0.1333\n",
      "\n",
      "Epoch 23/50\n",
      "----------\n",
      "train Loss: 1.9935 Acc: 0.1392\n",
      "valid Loss: 1.9795 Acc: 0.1333\n",
      "\n",
      "Epoch 24/50\n",
      "----------\n",
      "train Loss: 1.9900 Acc: 0.1392\n",
      "valid Loss: 1.9989 Acc: 0.1333\n",
      "\n",
      "Epoch 25/50\n",
      "----------\n",
      "train Loss: 2.0064 Acc: 0.1333\n",
      "valid Loss: 2.0130 Acc: 0.1333\n",
      "\n",
      "Epoch 26/50\n",
      "----------\n",
      "train Loss: 2.0176 Acc: 0.1333\n",
      "valid Loss: 2.0198 Acc: 0.1333\n",
      "\n",
      "Epoch 27/50\n",
      "----------\n",
      "train Loss: 2.0197 Acc: 0.1333\n",
      "valid Loss: 2.0171 Acc: 0.1333\n",
      "\n",
      "Epoch 28/50\n",
      "----------\n",
      "train Loss: 2.0135 Acc: 0.1333\n",
      "valid Loss: 2.0075 Acc: 0.1333\n",
      "\n",
      "Epoch 29/50\n",
      "----------\n",
      "train Loss: 2.0018 Acc: 0.1550\n",
      "valid Loss: 1.9952 Acc: 0.2000\n",
      "\n",
      "Epoch 30/50\n",
      "----------\n",
      "train Loss: 1.9883 Acc: 0.2000\n",
      "valid Loss: 1.9845 Acc: 0.2000\n",
      "\n",
      "Epoch 31/50\n",
      "----------\n",
      "train Loss: 1286.1405 Acc: 0.1517\n",
      "valid Loss: 1.9424 Acc: 0.2083\n",
      "\n",
      "Epoch 32/50\n",
      "----------\n",
      "train Loss: 1.9516 Acc: 0.1783\n",
      "valid Loss: 1.9438 Acc: 0.2000\n",
      "\n",
      "Epoch 33/50\n",
      "----------\n",
      "train Loss: 1.9434 Acc: 0.1983\n",
      "valid Loss: 1.9434 Acc: 0.2000\n",
      "\n",
      "Epoch 34/50\n",
      "----------\n",
      "train Loss: 1.9434 Acc: 0.2000\n",
      "valid Loss: 1.9430 Acc: 0.2000\n",
      "\n",
      "Epoch 35/50\n",
      "----------\n",
      "train Loss: 1.9424 Acc: 0.2000\n",
      "valid Loss: 1.9424 Acc: 0.2000\n",
      "\n",
      "Epoch 36/50\n",
      "----------\n",
      "train Loss: 1.9423 Acc: 0.2000\n",
      "valid Loss: 1.9423 Acc: 0.2000\n",
      "\n",
      "Epoch 37/50\n",
      "----------\n",
      "train Loss: 1.9422 Acc: 0.2000\n",
      "valid Loss: 1.9422 Acc: 0.2000\n",
      "\n",
      "Epoch 38/50\n",
      "----------\n",
      "train Loss: 1.9424 Acc: 0.2000\n",
      "valid Loss: 1.9420 Acc: 0.2000\n",
      "\n",
      "Epoch 39/50\n",
      "----------\n",
      "train Loss: 1.9420 Acc: 0.2000\n",
      "valid Loss: 1.9419 Acc: 0.2000\n",
      "\n",
      "Epoch 40/50\n",
      "----------\n",
      "train Loss: 1.9418 Acc: 0.2000\n",
      "valid Loss: 1.9417 Acc: 0.2000\n",
      "\n",
      "Epoch 41/50\n",
      "----------\n",
      "train Loss: 2.3077 Acc: 0.1767\n",
      "valid Loss: 1.9442 Acc: 0.2000\n",
      "\n",
      "Epoch 42/50\n",
      "----------\n",
      "train Loss: 1.9530 Acc: 0.1800\n",
      "valid Loss: 1.9433 Acc: 0.2000\n",
      "\n",
      "Epoch 43/50\n",
      "----------\n",
      "train Loss: 1.9458 Acc: 0.1800\n",
      "valid Loss: 1.9427 Acc: 0.2000\n",
      "\n",
      "Epoch 44/50\n",
      "----------\n",
      "train Loss: 1.9420 Acc: 0.2000\n",
      "valid Loss: 1.9427 Acc: 0.2000\n",
      "\n",
      "Epoch 45/50\n",
      "----------\n",
      "train Loss: 1.9412 Acc: 0.1950\n",
      "valid Loss: 1.9427 Acc: 0.2000\n",
      "\n",
      "Epoch 46/50\n",
      "----------\n",
      "train Loss: 1.9425 Acc: 0.2000\n",
      "valid Loss: 1.9424 Acc: 0.2000\n",
      "\n",
      "Epoch 47/50\n",
      "----------\n",
      "train Loss: 1.9423 Acc: 0.2000\n",
      "valid Loss: 1.9419 Acc: 0.2000\n",
      "\n",
      "Epoch 48/50\n",
      "----------\n",
      "train Loss: 1.9428 Acc: 0.2000\n",
      "valid Loss: 1.9415 Acc: 0.2000\n",
      "\n",
      "Epoch 49/50\n",
      "----------\n",
      "train Loss: 1.9419 Acc: 0.2000\n",
      "valid Loss: 1.9411 Acc: 0.2000\n",
      "\n",
      "Epoch 50/50\n",
      "----------\n",
      "train Loss: 1.9411 Acc: 0.2000\n",
      "valid Loss: 1.9408 Acc: 0.2000\n",
      "\n",
      "Training complete in 6m 21s\n",
      "Best val Acc: 0.208333\n"
     ]
    }
   ],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 10 epochs\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "\n",
    "model, loss_list, acc_list = train_model(model, criterion, optimizer,scheduler, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"./fusion1_CNN_acc.txt\"\n",
    "import json\n",
    "with open(filepath, 'w') as f:\n",
    "    f.write(json.dumps(acc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"./fusion1_CNN_loss.txt\"\n",
    "import json\n",
    "with open(filepath, 'w') as f:\n",
    "    f.write(json.dumps(loss_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two weights for output from audio network or facial network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'cuda'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-ef2e964c8bd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnum_ftrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimage_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_ftrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mimage_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./model_image30.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'cuda'"
     ]
    }
   ],
   "source": [
    "# image_model\n",
    "image_model = models.alexnet(pretrained=True)\n",
    "num_ftrs = image_model.classifier[6].in_features\n",
    "image_model.classifier[6] = nn.Linear(num_ftrs,7)\n",
    "image_model.load_state_dict(torch.load(\"./model_image30.pth\")).cuda()\n",
    "print(image_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'cuda'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-529d06baf361>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0maudio_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0maudio_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./model_audio_combine50.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'cuda'"
     ]
    }
   ],
   "source": [
    "# audio_model\n",
    "\n",
    "class Net(nn.Module):            \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 128, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(128, 128, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, 7),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "\n",
    "audio_model = net\n",
    "audio_model.load_state_dict(torch.load(\"./model_audio_combine50.pth\")).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Defining a model model which will do convolution fusion of both stream and 3D pooling \n",
    "'''\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.image_feature = image_model #feature_size =   Nx512x7x7\n",
    "        self.audio_feature = audio_model #feature_size = Nx512x7x7\n",
    "        #self.layer1       = nn.Sequential(nn.Conv3d(1024, 512, 1, stride=1, padding=1, dilation=1,bias=True),\n",
    "        #                          nn.ReLU(),nn.MaxPool3d(kernel_size=2,stride=2))\n",
    "        self.fc           = nn.Sequential(nn.Linear(2,1))\n",
    "        \n",
    "    def forward(self,image_data,audio_data):\n",
    "        x1       = self.image_feature(image_data)\n",
    "        x2       = self.audio_feature(audio_data)\n",
    "        #print(x1)\n",
    "        #print(x2.shape)\n",
    "        #print(\"image_preds: \", torch.max(x1,1))\n",
    "        #print(\"audio_preds: \", torch.max(x2,1))\n",
    "        #x1[:,0].view(16,1)\n",
    "        \n",
    "        temp = torch.cat([x1[:,0].view(16,1),x2[:,0].view(16,1)], dim = 1)\n",
    "        output = self.fc(temp)\n",
    "        for i in range(1,7):\n",
    "            temp = torch.cat([x1[:,i].view(16,1),x2[:,i].view(16,1)], dim = 1)\n",
    "            temp_output = self.fc(temp)\n",
    "            #print(temp_output)\n",
    "            output = torch.cat((output,temp_output),dim = 1)\n",
    "        #print(output.shape)\n",
    "        \n",
    "        #print(y1.shape)\n",
    "        \n",
    "        #y        = torch.cat((x1,x2), dim= 1)\n",
    "\n",
    "        '''\n",
    "        for i in range(x1.size(1)):\n",
    "            y[:,(2*i),:,:]   = x1[:,i,:,:]\n",
    "            y[:,(2*i+1),:,:] = x2[:,i,:,:]\n",
    "            \n",
    "        y        = y.view(y.size(0), 1024, 1, 7, 7)\n",
    "        cnn_out  = self.layer1(y)\n",
    "        cnn_out  = cnn_out.view(cnn_out.size(0),-1)    \n",
    "        '''\n",
    "        \n",
    "            \n",
    "        \n",
    "        #out      = self.fc(y)\n",
    "        #print(out.shape)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets freeze the first few layers. This is done in two stages \n",
    "# Stage-1 Freezing all the layers \n",
    "freeze_layers = 1\n",
    "if freeze_layers:\n",
    "    for i, param in model.named_parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "# Stage-2 , Freeze all the layers till \"Conv2d_4a_3*3\"\n",
    "ct = []\n",
    "for name, param in model.named_parameters():\n",
    "    if \"audio_feature.classifier.6.bias\" in ct:\n",
    "        param.requires_grad = True\n",
    "    ct.append(name)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t fc.0.weight\n",
      "\t fc.0.bias\n"
     ]
    }
   ],
   "source": [
    "# Create the optimizer if freeze layer before\n",
    "params_to_update = model.parameters()\n",
    "print(\"Params to learn:\")\n",
    "freeze_layers = 1\n",
    "if freeze_layers:\n",
    "    params_to_update = []\n",
    "    for name,param in model.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "            print(\"\\t\",name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 2.9289 Acc: 0.5450\n",
      "valid Loss: 9.0509 Acc: 0.5167\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 5.7634 Acc: 0.6200\n",
      "valid Loss: 11.3355 Acc: 0.5250\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 5.4615 Acc: 0.5750\n",
      "valid Loss: 7.6789 Acc: 0.4792\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 6.0189 Acc: 0.4250\n",
      "valid Loss: 18.3082 Acc: 0.1875\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 7.7404 Acc: 0.5558\n",
      "valid Loss: 19.5111 Acc: 0.4833\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 18.1188 Acc: 0.5392\n",
      "valid Loss: 40.5474 Acc: 0.4833\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 28.3080 Acc: 0.5183\n",
      "valid Loss: 48.5281 Acc: 0.4958\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 27.8126 Acc: 0.6050\n",
      "valid Loss: 51.5499 Acc: 0.5167\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 28.4026 Acc: 0.6025\n",
      "valid Loss: 48.3550 Acc: 0.5167\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 26.5622 Acc: 0.6058\n",
      "valid Loss: 39.1216 Acc: 0.4833\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 22.4032 Acc: 0.6100\n",
      "valid Loss: 35.9485 Acc: 0.4875\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 5.6793 Acc: 0.5858\n",
      "valid Loss: 8.3203 Acc: 0.5000\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 3.9545 Acc: 0.5575\n",
      "valid Loss: 5.1831 Acc: 0.4750\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 4.0379 Acc: 0.2725\n",
      "valid Loss: 2.0243 Acc: 0.3750\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 4.5749 Acc: 0.3817\n",
      "valid Loss: 12.4673 Acc: 0.2542\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 8.2124 Acc: 0.4250\n",
      "valid Loss: 12.3591 Acc: 0.3708\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 6.7452 Acc: 0.5858\n",
      "valid Loss: 13.0451 Acc: 0.5000\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 8.2138 Acc: 0.6133\n",
      "valid Loss: 16.5273 Acc: 0.5208\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 10.9056 Acc: 0.6058\n",
      "valid Loss: 20.0346 Acc: 0.5167\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 12.4016 Acc: 0.6142\n",
      "valid Loss: 22.8497 Acc: 0.5083\n",
      "\n",
      "Training complete in 2m 24s\n",
      "Best val Acc: 0.525000\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 10 epochs\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "\n",
    "model = train_model(model, criterion, optimizer,scheduler, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.2851, 0.3746]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.fc[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
